{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79dfc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = 'processced df.csv'\n",
    "OUTPUT = 'processced text.csv'\n",
    "TFIDF_MAT=\"tfidf_matrix.npz\"\n",
    "BOW_MAT=\"bow_matrix.npz\"\n",
    "COUNT_VEC='out/count_vectorizer.pkl'\n",
    "TFIDF_VEC=\"out/tfidf_vectorizer.pkl\"\n",
    "# INPUT=\"../api/feb2024dataset/processed_articles.csv\"\n",
    "# OUTPUT=\"../api/feb2024dataset/processed_text.csv\"\n",
    "# TFIDF_MAT=\"../api/feb2024dataset/tfidf_matrix.npz\"\n",
    "# BOW_MAT=\"../api/feb2024dataset/bow_matrix.npz\"\n",
    "# COUNT_VEC=\"../api/feb2024dataset/out/count_vectorizer.pkl\"\n",
    "# TFIDF_VEC=\"../api/feb2024dataset/out/tfidf_vectorizer.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56631bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48a8ee5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>news_desk</th>\n",
       "      <th>section_name</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>Learning</td>\n",
       "      <td>The Learning Network</td>\n",
       "      <td>This word has appeared in 993 articles on NYTi...</td>\n",
       "      <td>The Learning Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>Arts &amp; Culture</td>\n",
       "      <td>Movies</td>\n",
       "      <td>With the victory, the Christopher Nolan biopic...</td>\n",
       "      <td>Kyle Buchanan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quote</td>\n",
       "      <td>Summary</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>Quotation of the Day for Monday, February 26, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>No corrections appeared in print on Monday, Fe...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News</td>\n",
       "      <td>Arts &amp; Culture</td>\n",
       "      <td>Arts</td>\n",
       "      <td>FX premieres a new show set in Japan in the 16...</td>\n",
       "      <td>Shivani Gonzalez</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type_of_material       news_desk          section_name  \\\n",
       "0             News        Learning  The Learning Network   \n",
       "1             News  Arts & Culture                Movies   \n",
       "2            Quote         Summary           Corrections   \n",
       "3             News     Corrections           Corrections   \n",
       "4             News  Arts & Culture                  Arts   \n",
       "\n",
       "                                       combined_text                author  \n",
       "0  This word has appeared in 993 articles on NYTi...  The Learning Network  \n",
       "1  With the victory, the Christopher Nolan biopic...         Kyle Buchanan  \n",
       "2  Quotation of the Day for Monday, February 26, ...                   NaN  \n",
       "3  No corrections appeared in print on Monday, Fe...                   NaN  \n",
       "4  FX premieres a new show set in Japan in the 16...      Shivani Gonzalez  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(INPUT)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf072374",
   "metadata": {},
   "source": [
    "### Text Preprocessing Function: Combined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05225578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    # remove special characters, punctuation, and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # tokenize and lemmatize the text\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if token.text not in stopwords.words('english')]\n",
    "    # join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# apply this function to 'combined_text' \n",
    "df['preprocessed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "df = df.drop('combined_text', axis=1)\n",
    "# Now the 'preprocessed_text' column will have the cleaned and preprocessed text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c6f58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>news_desk</th>\n",
       "      <th>section_name</th>\n",
       "      <th>author</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>Learning</td>\n",
       "      <td>The Learning Network</td>\n",
       "      <td>The Learning Network</td>\n",
       "      <td>word appear   article nytimescom past year use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>Arts &amp; Culture</td>\n",
       "      <td>Movies</td>\n",
       "      <td>Kyle Buchanan</td>\n",
       "      <td>victory christopher nolan biopic sweep guild p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quote</td>\n",
       "      <td>Summary</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>NaN</td>\n",
       "      <td>quotation day monday february    understand fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>NaN</td>\n",
       "      <td>correction appear print monday feb    error co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News</td>\n",
       "      <td>Arts &amp; Culture</td>\n",
       "      <td>Arts</td>\n",
       "      <td>Shivani Gonzalez</td>\n",
       "      <td>fx premiere new show set japan new miniserie s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type_of_material       news_desk          section_name  \\\n",
       "0             News        Learning  The Learning Network   \n",
       "1             News  Arts & Culture                Movies   \n",
       "2            Quote         Summary           Corrections   \n",
       "3             News     Corrections           Corrections   \n",
       "4             News  Arts & Culture                  Arts   \n",
       "\n",
       "                 author                                  preprocessed_text  \n",
       "0  The Learning Network  word appear   article nytimescom past year use...  \n",
       "1         Kyle Buchanan  victory christopher nolan biopic sweep guild p...  \n",
       "2                   NaN  quotation day monday february    understand fl...  \n",
       "3                   NaN  correction appear print monday feb    error co...  \n",
       "4      Shivani Gonzalez  fx premiere new show set japan new miniserie s...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26005d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fx premiere new show set japan new miniserie star kate winslet air hbo like still not cut cord selection cable network tv show movie special broadcast week feb march   detail time subject change tv week shogun regime television movie\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[4, \"preprocessed_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f84c647",
   "metadata": {},
   "source": [
    "### Vectorization: Using BoW to Transform Preprocessed Text Data for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a69b9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#create a countvectorizer instance\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "#apply it to preprocessed text\n",
    "bow_matrix = vectorizer.fit_transform(df['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820d226",
   "metadata": {},
   "source": [
    "### Vectorization: Using TF-IDF to Transform Preprocessed Text Data for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b33ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# instantiate the TF-IDF \n",
    "tfidf_vect = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "\n",
    "# apply it to preprocessed text\n",
    "tfidf_matrix = tfidf_vect.fit_transform(df['preprocessed_text'])\n",
    "\n",
    "# tfidf_matrix is a sparse matrix representation of documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a4b23",
   "metadata": {},
   "source": [
    "### \"tfidf_matrix\" is what we will use for Topic Modeling and be served as predictors for the topic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2645e0",
   "metadata": {},
   "source": [
    "### To better understand the \"tfidf_matrix\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86113be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abortion</th>\n",
       "      <th>abuse</th>\n",
       "      <th>academy</th>\n",
       "      <th>accident</th>\n",
       "      <th>accord</th>\n",
       "      <th>account</th>\n",
       "      <th>accuse</th>\n",
       "      <th>across</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>writer</th>\n",
       "      <th>writing</th>\n",
       "      <th>year</th>\n",
       "      <th>yearold</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>yourfeedscience</th>\n",
       "      <th>yulia</th>\n",
       "      <th>zelensky</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.230155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.40839</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abortion  abuse  academy  accident  accord  account  accuse  across  act  \\\n",
       "0       0.0    0.0  0.00000       0.0     0.0      0.0     0.0     0.0  0.0   \n",
       "1       0.0    0.0  0.40839       0.0     0.0      0.0     0.0     0.0  0.0   \n",
       "2       0.0    0.0  0.00000       0.0     0.0      0.0     0.0     0.0  0.0   \n",
       "3       0.0    0.0  0.00000       0.0     0.0      0.0     0.0     0.0  0.0   \n",
       "4       0.0    0.0  0.00000       0.0     0.0      0.0     0.0     0.0  0.0   \n",
       "\n",
       "   action  ...  writer  writing      year  yearold  yet  york  young  \\\n",
       "0     0.0  ...     0.0      0.0  0.230155      0.0  0.0   0.0    0.0   \n",
       "1     0.0  ...     0.0      0.0  0.000000      0.0  0.0   0.0    0.0   \n",
       "2     0.0  ...     0.0      0.0  0.000000      0.0  0.0   0.0    0.0   \n",
       "3     0.0  ...     0.0      0.0  0.000000      0.0  0.0   0.0    0.0   \n",
       "4     0.0  ...     0.0      0.0  0.000000      0.0  0.0   0.0    0.0   \n",
       "\n",
       "   yourfeedscience  yulia  zelensky  \n",
       "0              0.0    0.0       0.0  \n",
       "1              0.0    0.0       0.0  \n",
       "2              0.0    0.0       0.0  \n",
       "3              0.0    0.0       0.0  \n",
       "4              0.0    0.0       0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view as dense matrix\n",
    "dense_tfidf = tfidf_matrix[:5].todense()\n",
    "pd.DataFrame(dense_tfidf, columns=tfidf_vect.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f445ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tfidf_matrix: (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# display the shape of the matrix\n",
    "print(\"Shape of tfidf_matrix:\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3017edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-zero TF-IDF values in document 0:\n",
      "appear: 0.33369204358556975\n",
      "article: 0.3843633401735585\n",
      "day: 0.1390727267593142\n",
      "include: 0.1643407977009323\n",
      "michael: 0.21776256267191377\n",
      "past: 0.36720255224701315\n",
      "real: 0.17335980805256246\n",
      "think: 0.18626823639743526\n",
      "use: 0.15368638573345986\n",
      "word: 0.609070472184471\n",
      "year: 0.23015483787537938\n"
     ]
    }
   ],
   "source": [
    "# print some non-0 TF-IDF values of the first document\n",
    "doc = 0  \n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "print(f\"Non-zero TF-IDF values in document {doc}:\")\n",
    "for word, tfidf_value in zip(feature_names, tfidf_matrix[doc].toarray().flatten()):\n",
    "    if tfidf_value > 0:\n",
    "        print(f\"{word}: {tfidf_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca28598",
   "metadata": {},
   "source": [
    "### Other Features\n",
    "\n",
    "How to use other features for the topic modeling task or the topic classification task, depending on the nature of these features and the goals of our analysis. \n",
    "\n",
    "We will get back to \"other features\" when we do our topic modeling task. \n",
    "\n",
    "We intend to use other features as predictors for our topic classification task. But we need to wait until we obtain the topic labels from our topic modeling results. As we will need to evaluate  which \"other features\" have a meaningful relationship with the topic labels. we may only choose one of them, or combination of few of them based on the consideration of the model complexity and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed31bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataset with the preprocessed text\n",
    "df.to_csv(OUTPUT, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01bde037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# save tfidf_matrix\n",
    "sparse.save_npz(TFIDF_MAT, tfidf_matrix)\n",
    "sparse.save_npz(BOW_MAT, bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8546cd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "022c9e7e",
   "metadata": {},
   "source": [
    "# Pickling the vectorizer to use in different notebooks\n",
    "\n",
    "To analyze the different topics visually - we can print the words associated to a topic. To do this, we need access to the vectorizer instance across different jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a20f163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(COUNT_VEC, 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "312c6450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(TFIDF_VEC, 'wb') as f:\n",
    "    pickle.dump(tfidf_vect, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa0d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
