{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9152196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\qfu88\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\qfu88\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from scipy.stats import chi2_contingency\n",
    "from gensim import corpora, models\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import spacy\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89732795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>news_desk</th>\n",
       "      <th>section_name</th>\n",
       "      <th>author</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News</td>\n",
       "      <td>Lifestyle &amp; Leisure</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>Penelope Green</td>\n",
       "      <td>point view   prescriptive descriptive definite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News</td>\n",
       "      <td>Lifestyle &amp; Leisure</td>\n",
       "      <td>Home &amp; Garden</td>\n",
       "      <td>Marianne Rohrlich</td>\n",
       "      <td>ten highend design gallery offer     percent m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News</td>\n",
       "      <td>Business</td>\n",
       "      <td>Business Day</td>\n",
       "      <td>Lauren Laughlin</td>\n",
       "      <td>banker may suffer decline dealmaking also take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News</td>\n",
       "      <td>National</td>\n",
       "      <td>U.S.</td>\n",
       "      <td>Damien Cave</td>\n",
       "      <td>florida scale back purchase sugar company land...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News</td>\n",
       "      <td>Business</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Bloomberg News</td>\n",
       "      <td>taketwo interactive maker grand theft auto acc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type_of_material            news_desk   section_name             author  \\\n",
       "0             News  Lifestyle & Leisure  Home & Garden     Penelope Green   \n",
       "1             News  Lifestyle & Leisure  Home & Garden  Marianne Rohrlich   \n",
       "2             News             Business   Business Day    Lauren Laughlin   \n",
       "3             News             National           U.S.        Damien Cave   \n",
       "4             News             Business     Technology     Bloomberg News   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  point view   prescriptive descriptive definite...  \n",
       "1  ten highend design gallery offer     percent m...  \n",
       "2  banker may suffer decline dealmaking also take...  \n",
       "3  florida scale back purchase sugar company land...  \n",
       "4  taketwo interactive maker grand theft auto acc...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('processced text.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c224ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taketwo interactive maker grand theft auto accuse backdate option historically low price scheme reward key employee taketwo interactive software maker grand theft auto video game agree pay   million settle lawsuit security exchange commission accuse company backdate stock option video game maker pay   million settle stock option case suit litigation taketwo interactive software inc stock option purchase plan security commodity violation\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[4, \"preprocessed_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f2c96",
   "metadata": {},
   "source": [
    "### \"Best Model\" used on the newly preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0775e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove common words\n",
    "#list of common words to remove\n",
    "common_words_to_remove = [\"new\", \"york\", \"year\", \"city\"]\n",
    "\n",
    "# define a function to remove those words from \"preprocessed_text\" feature\n",
    "def remove_common_words(text, words_to_remove):\n",
    "    tokens = text.split()  \n",
    "    filtered_tokens = [token for token in tokens if token not in words_to_remove]\n",
    "    return ' '.join(filtered_tokens)  \n",
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda text: remove_common_words(text, common_words_to_remove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "616c2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized the newly preprocessed data\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
    "\n",
    "# apply it to preprocessed text\n",
    "tfidf_matrix = tfidf_vect.fit_transform(df['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef94cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lambda x: x.split())\n",
    "texts = df['preprocessed_text'].tolist()\n",
    "# create a gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f882b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4c66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random state\n",
    "random_state = 696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f61b48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qfu88\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator NMF from version 1.4.0 when using version 1.1.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "with open('out/best_model.pkl', 'rb') as g:\n",
    "    best_model = pickle.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f738338",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NMF' object has no attribute 'alpha'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidf_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m H \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mcomponents_\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1593\u001b[0m, in \u001b[0;36mNMF.fit_transform\u001b[1;34m(self, X, y, W, H)\u001b[0m\n\u001b[0;32m   1588\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m   1589\u001b[0m     X, accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[0;32m   1590\u001b[0m )\n\u001b[0;32m   1592\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(assume_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 1593\u001b[0m     W, H, n_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruction_err_ \u001b[38;5;241m=\u001b[39m _beta_divergence(\n\u001b[0;32m   1596\u001b[0m     X, W, H, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beta_loss, square_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1597\u001b[0m )\n\u001b[0;32m   1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_ \u001b[38;5;241m=\u001b[39m H\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1642\u001b[0m, in \u001b[0;36mNMF._fit_transform\u001b[1;34m(self, X, y, W, H, update_H)\u001b[0m\n\u001b[0;32m   1639\u001b[0m check_non_negative(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNMF (input X)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;66;03m# check parameters\u001b[39;00m\n\u001b[1;32m-> 1642\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mmin() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beta_loss \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1646\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen beta_loss <= 0 and X contains zeros, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1647\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe solver may diverge. Please add small values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1648\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto X, or use a positive beta_loss.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1649\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dojo-env\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1476\u001b[0m, in \u001b[0;36mNMF._check_params\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multiplicative update (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) solver cannot update \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1468\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros present in the initialization, and so leads to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;66;03m# alpha and regularization are deprecated in favor of alpha_W and alpha_H\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;66;03m# TODO clean up in 1.2\u001b[39;00m\n\u001b[1;32m-> 1476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1477\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`alpha` was deprecated in version 1.0 and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1479\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 1.2. Use `alpha_W` and `alpha_H` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1480\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1481\u001b[0m     )\n\u001b[0;32m   1482\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NMF' object has no attribute 'alpha'"
     ]
    }
   ],
   "source": [
    "W = best_model.fit_transform(tfidf_matrix)\n",
    "H = best_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0f6ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the coherence score.\n",
    "num_top_words = 50\n",
    "topics = []\n",
    "\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    top_features_ind = topic.argsort()[-num_top_words:][::-1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    topics.append(top_features)\n",
    "\n",
    "\n",
    "coherence_model = CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_score = coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc07b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be46ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 50 words of each topic\n",
    "num_top_words = 100\n",
    "\n",
    "for topic_idx, topic in enumerate(H):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    top_features_ind = topic.argsort()[-num_top_words:][::-1]  \n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    top_features_str = \", \".join(top_features)\n",
    "    print(top_features_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9fa39",
   "metadata": {},
   "source": [
    "### Descriptive label for each topic and article-topic distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18ce9c",
   "metadata": {},
   "source": [
    "Topic 1: Cultural, Social, and Urban Life\n",
    "\n",
    "Topic 2: Personal Events and Ceremonies\n",
    "\n",
    "Topic 3: Politics, Governance, and International Affairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the dominant topic for each article\n",
    "dominant_topic = np.argmax(W, axis=1)\n",
    "\n",
    "# Count the number of articles associated with each topic\n",
    "topic_counts = np.bincount(dominant_topic)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.pie(topic_counts, labels=['Topic 1', 'Topic 2', 'Topic 3'], autopct='%1.1f%%', startangle=140, colors=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Proportion of Articles by Dominant Topic')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cea6c64",
   "metadata": {},
   "source": [
    "### Assigning the topic label to each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ada65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add topic label column\n",
    "df['topic_label'] = dominant_topic\n",
    "\n",
    "topic_names = {0: 'Lifestyle', 1: 'Events', 2: 'Politics'}\n",
    "\n",
    "df['topic_label'] = df['topic_label'].map(topic_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ac32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941e45a",
   "metadata": {},
   "source": [
    "## Examine the predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53500cf5",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "perform a chi-square test of independence to see if there is a significant association between each of these predictors and the topic_label.\n",
    "\n",
    "The Chi-square test will help us understand whether the distribution of topic labels is independent of these categorical variables or if there's a significant association between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c771bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert to num\n",
    "df['type_of_material_num'] = df['type_of_material'].astype('category').cat.codes\n",
    "df['news_desk_num'] = df['news_desk'].astype('category').cat.codes\n",
    "df['section_name_num'] = df['section_name'].astype('category').cat.codes\n",
    "df['author_num'] = df['author'].astype('category').cat.codes\n",
    "\n",
    "# perform chi-square test for each feature against 'topic_label'\n",
    "features = ['type_of_material_num', 'news_desk_num', 'section_name_num', 'author_num']\n",
    "for feature in features:\n",
    "    contingency_table = pd.crosstab(df[feature], df['topic_label'])\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    print(f\"Chi-square test for {feature} vs topic_label:\")\n",
    "    print(f\"Chi-square statistic: {chi2}, p-value: {p}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea02df",
   "metadata": {},
   "source": [
    "### Interpreting the result\n",
    "\n",
    "all four features (type_of_material, news_desk, section_name, author) show a statistically significant association with the topic_label. This means these features could potentially be useful predictors in our topic classification task. Their inclusion could improve the model's ability to accurately predict the topic of an article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f38da",
   "metadata": {},
   "source": [
    "### To avoid high dimentionality in the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Value Counts for 'type_of_material':\")\n",
    "print(df['type_of_material'].value_counts())\n",
    "\n",
    "print(\"\\nValue Counts for 'news_desk':\")\n",
    "print(df['news_desk'].value_counts())\n",
    "\n",
    "print(\"\\nValue Counts for 'section_name':\")\n",
    "print(df['section_name'].value_counts())\n",
    "\n",
    "print(\"\\nValue Counts for 'author':\")\n",
    "print(df['author'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d8bec",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Strategy:\n",
    "\n",
    "Based on the distribution of each feature, identifying the top n categories for each feature based on their frequency and then grouping the less frequent categories into an \"Other\" category. \n",
    "\n",
    "Top 20 authors\n",
    "Top 10 news desks\n",
    "Top 10 section names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ac2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_authors = df['author'].value_counts().nlargest(20).index\n",
    "top_news_desks = df['news_desk'].value_counts().nlargest(10).index\n",
    "top_section_names = df['section_name'].value_counts().nlargest(10).index\n",
    "\n",
    "# group other categories into 'Other'\n",
    "df['author_reduced'] = df['author'].apply(lambda x: 'Other' if x not in top_authors else x)\n",
    "df['news_desk_reduced'] = df['news_desk'].apply(lambda x: 'Other' if x not in top_news_desks else x)\n",
    "df['section_name_reduced'] = df['section_name'].apply(lambda x: 'Other' if x not in top_section_names else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d855e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed345d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop no longer needed columns\n",
    "df_model = df.drop(['news_desk', 'section_name', 'author', \n",
    "                    'type_of_material_num', 'news_desk_num', 'section_name_num', 'author_num'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e46ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2536250",
   "metadata": {},
   "source": [
    "## Getting df ready for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3448be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "df_encoded = pd.get_dummies(df_model, columns=['type_of_material', 'author_reduced', 'news_desk_reduced', 'section_name_reduced'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1884b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize preprocessed_text\n",
    "\n",
    "# convert list of words in 'preprocessed_text' to a string\n",
    "df_encoded['preprocessed_text'] = df_encoded['preprocessed_text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000) \n",
    "\n",
    "# Fit and transform 'preprocessed_text' to obtain TF-IDF features\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_encoded['preprocessed_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2c6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine tfid_matrix with encoded features\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# drop 'preprocessed_text' \n",
    "df_encoded.drop('preprocessed_text', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "df_final = pd.concat([df_encoded.reset_index(drop=True), tfidf_df.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0026db46",
   "metadata": {},
   "source": [
    "#### Now, the df_final is the one used for topic classification models, containing both the vectorized preprocessed_text predictor and one-hot encoded other predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b437be93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/df_final.pkl', 'wb') as f:\n",
    "    pickle.dump(df_final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bd248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/dominant_topic_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(dominant_topic, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
