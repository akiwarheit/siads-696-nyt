{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitivity Analysis \n",
    "\n",
    "The purpose of this analysis is to determine the optimal number of topics for the NYT dataset. By evaluating the coherence scores across different numbers of topics, we aim to identify the configuration that produces the most coherent and meaningful topics. This process helps in uncovering the underlying structure and themes within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from scipy import sparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_matrix = sparse.load_npz(\"bow_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/count_vectorizer.pkl', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out/lda_model.pkl', 'rb') as g:\n",
    "    lda_model = pickle.load(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the vocabulary\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_topic_words(lda_model=lda_model, num_top_words=50):\n",
    "    num_top_words = 50\n",
    "\n",
    "    topic_words = []\n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_words_idx = topic.argsort()[:-num_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topic_words.append(top_words)\n",
    "\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = vectorizer.get_feature_names_out()\n",
    "\n",
    "df = pd.read_csv(\"processced text.csv\")\n",
    "\n",
    "dictionary = Dictionary(df['preprocessed_text'].apply(lambda x: list(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "def calc_coherence(topic_idx, words):\n",
    "    coherence_model_lda = CoherenceModel(\n",
    "        topics=words, texts=df['preprocessed_text'], dictionary=dictionary, coherence='c_v'\n",
    "    )\n",
    "    return topic_idx, coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def train_lda(num_topics):\n",
    "    lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=10, random_state=42)\n",
    "    lda_model.fit(bow_matrix)\n",
    "    topic_words = get_topic_words(lda_model)\n",
    "\n",
    "    total_score = 0\n",
    "    for topic in topic_words:\n",
    "        _, score=  calc_coherence(num_topics, topic)\n",
    "        total_score += score\n",
    "\n",
    "    avg_coherence_score = total_score / num_topics\n",
    "\n",
    "    return num_topics, {'model': lda_model, 'avg_coherence_score': avg_coherence_score }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_map = {}\n",
    "\n",
    "for num_topics in range(2, 9):\n",
    "    _x, m_score = train_lda(num_topics)\n",
    "\n",
    "    model_score_map[_x] = m_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting x and y values\n",
    "x = list(model_score_map.keys())\n",
    "y = [entry['avg_coherence_score'] for entry in model_score_map.values()]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, marker='o', color='b', linestyle='-')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Average Coherence Score')\n",
    "plt.title('Average Coherence Score for Different Numbers of Topics')\n",
    "plt.xticks(x)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "## Average Coherence Score Trend\n",
    "* As the number of topics increases from 2 to 8, there is a slight fluctuation in the average coherence score.\n",
    "* The average coherence score ranges from approximately `0.464` to `0.472`, indicating a relatively consistent performance across different numbers of topics.\n",
    "## Optimal Number of Topics\n",
    "* The highest average coherence score is achieved with 2 topics, with a score of approximately `0.472`.\n",
    "* As the number of topics increases beyond 2, there is a slight decrease in the average coherence score, with scores ranging between `0.463` and `0.468` for 6 to 7 topics.\n",
    "## Interpretability vs. Granularity:\n",
    "* A lower number of topics (e.g., 2) may result in higher coherence scores, indicating more interpretable and distinct topics.\n",
    "* However, increasing the number of topics beyond a certain threshold (e.g., 6 or 7) may lead to a decrease in coherence scores, suggesting that the topics become more granular and potentially less interpretable.\n",
    "## Robustness\n",
    "* The LDA model demonstrates robustness to changes in the number of topics, as the average coherence scores remain relatively close to each other across different configurations.\n",
    "* This suggests that the model is not overly sensitive to small variations in the number of topics within the specified range.\n",
    "## Considerations\n",
    "* The choice of the number of topics should consider a balance between coherence and granularity. A higher coherence score does not necessarily imply better topic quality if the topics become too broad or overlapping.\n",
    "* It's essential to evaluate the coherence scores in conjunction with qualitative assessments of the topics to ensure they are meaningful and interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So is `n_components=2` optimal?\n",
    "\n",
    "Using an LDA model with only 2 topics for classifying articles may not be the most effective approach for several reasons:\n",
    "\n",
    "1. With only 2 topics, the LDA model may struggle to capture the diverse range of topics present in NYT articles effectively. This lack of granularity can result in topics that are too broad or overlapping, making it challenging to accurately classify articles into meaningful categories resulting in the lack of topic differentation.\n",
    "\n",
    "2. Limiting the number of topics to 2 can lead to a significant loss of information. NYT covers a wide range of topics spanning politics, business, sports, entertainment, science, and more. A binary classification of articles into just two broad topics would not capture this richness of content adequately. This results in loss of information\n",
    "\n",
    "3. A binary classification model may lack the discriminative power needed to distinguish between closely related topics or subtopics within the same category. This can result in misclassification of articles and reduced performance in tasks such as recommendations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Similarity\n",
    "\n",
    "We want to quantify the degree of similarity or dissimilarity between topics identified by different LDA configurations. This allows for a direct comparison of topics generated by LDA models with different numbers of topics. By visualizing the similarity matrices, we can identify patterns and trends in how topics evolve as the number of topics varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def calc_lda_model_topic_similarity_matrix(idx):\n",
    "\n",
    "    model = model_score_map[idx]['model']\n",
    "\n",
    "    topic_words = get_topic_words(model)\n",
    "\n",
    "    bow = vectorizer.transform([' '.join(words) for words in topic_words])\n",
    "\n",
    "    similarity_matrix = []\n",
    "\n",
    "    for topic_1 in bow:\n",
    "        for topic_2 in bow:\n",
    "            similarity_matrix.append(cosine_similarity(topic_1, topic_2)[0][0])\n",
    "\n",
    "    similarity_matrix = np.matrix(similarity_matrix).reshape(len(topic_words), len(topic_words))\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the number of rows and columns for the subplot grid\n",
    "num_rows = 4\n",
    "num_cols = 2\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15))\n",
    "\n",
    "sim_idx = []\n",
    "\n",
    "# Iterate over each LDA model and display the similarity matrix heatmap\n",
    "for x in range(2, 9):\n",
    "    similarity_matrix = calc_lda_model_topic_similarity_matrix(x)\n",
    "    avg_sim = '{0:.4f}'.format(np.average(similarity_matrix))\n",
    "\n",
    "\n",
    "    sim_idx.append((x, avg_sim))\n",
    "\n",
    "    row = (x - 2) // num_cols\n",
    "    col = (x - 2) % num_cols\n",
    "    ax = axes[row, col]\n",
    "    sns.heatmap(similarity_matrix, annot=True, cmap=\"YlGnBu\", xticklabels=False, yticklabels=False, ax=ax)\n",
    "    ax.set_title(f\"LDA Model with {x} Topics: Topic Similarity Matrix. Avg Similarity: {avg_sim}\")\n",
    "    ax.set_xlabel(\"Topic Index\")\n",
    "    ax.set_ylabel(\"Topic Index\")\n",
    "\n",
    "\n",
    "print(sim_idx)\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "The LDA models were constructed with varying numbers of topics ranging from 2 to 8.\n",
    "\n",
    "* As the number of topics increases, the average similarity score decreases consistently. This suggests that as the LDA model becomes more granular with a higher number of topics, the topics become less similar to each other on average.\n",
    "\n",
    "* Higher similarity scores indicate a greater degree of overlap or coherence between topics, suggesting that the topics are more closely related or represent similar themes. Conversely, lower similarity scores imply that the topics are more distinct or diverse from each other.\n",
    "\n",
    "A higher similarity score may indicate a more cohesive set of topics, but it could also imply a lack of granularity. Conversely, a lower similarity score may suggest more distinct topics but could lead to potential overlaps or ambiguity in topic interpretation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
